{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNqQ02gEgJd34FCJ57wIGTw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Procesamiento Digital de Im√°genes\n","## üì∏ Clase Final: Generaci√≥n Autom√°tica de Descripciones de Im√°genes\n","\n","En esta √∫ltima clase vamos a explorar la generaci√≥n autom√°tica de descripciones para im√°genes.\n","\n","Vas a aprender a usar modelos de lenguaje visual (VLMs) para que le pongan un \"subt√≠tulo\" a tus im√°genes, y encima, ¬°en espa√±ol! La idea es que al final de la clase, tengas un sistema funcional que genere estas descripciones autom√°ticamente.\n","\n","---\n","### üéØ Objetivos de Aprendizaje\n","\n","Al final de esta clase, vas a poder:\n","\n","*   **Entender c√≥mo funcionan los VLMs (Modelos de Lenguaje-Visi√≥n):** Vas a comprender c√≥mo estos modelos combinan lo que \"ven\" con lo que \"entienden\" del lenguaje para generar texto.\n","*   **Generar subt√≠tulos autom√°ticamente:** Vamos a explorar c√≥mo se crean descripciones de im√°genes de forma aut√≥noma.\n","*   **Implementar un sistema completo:** Vas a armar un generador de descripciones de im√°genes que funcione en espa√±ol.\n","\n","---\n","### üìå Conceptos Clave\n","\n","Para seguir el hilo de la clase, ten√© en cuenta estos conceptos:\n","\n","*   **Image Captioning:** Imaginate que ten√©s una foto y quer√©s que una computadora le ponga un \"subt√≠tulo\" o una descripci√≥n. Bueno, eso es `Image Captioning`. Es el proceso autom√°tico de generar texto que describe el contenido visual de una imagen.\n","*   **BLIP (Bootstrap Language-Image Pre-training):** Este es el modelo de IA que vamos a usar para el `Image Captioning`. Es un modelo de `transformers` que fue entrenado con much√≠simas im√°genes y textos, lo que le permite entender la relaci√≥n entre lo visual y lo textual.\n","*   **Traducci√≥n Autom√°tica:** Como el modelo BLIP genera las descripciones en ingl√©s, vamos a usar otro modelo para traducir autom√°ticamente esas descripciones al espa√±ol. Es como tener un traductor instant√°neo para nuestro sistema."],"metadata":{"id":"RzESv397Vp3i"}},{"cell_type":"code","source":["# @title Instalaci√≥n de Dependencias\n","\n","# Antes de empezar a codear, tenemos que instalar algunas librer√≠as clave que vamos a usar.\n","# Pensalo como preparar la \"caja de herramientas\" para nuestro proyecto.\n","\n","# Vamos a correr estos comandos en tu Google Colab:\n","!pip install transformers\n","!pip install gradio\n","!pip install Pillow\n","!pip install accelerate -U\n","!pip install sentencepiece"],"metadata":{"id":"DzfGGEhRWZmA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*   `transformers`: Es la librer√≠a principal de Hugging Face, nos va a permitir usar modelos de Inteligencia Artificial pre-entrenados para tareas de lenguaje y visi√≥n.\n","*   `gradio`: Con esta librer√≠a, vas a poder crear una interfaz web s√∫per sencilla para que cualquiera pueda probar tu modelo sin saber nada de c√≥digo.\n","*   `Pillow`: Es la librer√≠a de procesamiento de im√°genes por excelencia en Python. La vamos a usar para cargar y manipular las fotos.\n","*   `accelerate`: Ayuda a que los modelos grandes de `transformers` corran m√°s r√°pido, especialmente si ten√©s acceso a una GPU.\n","*   `sentencepiece`: Es una dependencia que necesitan algunos de los modelos de lenguaje para tokenizar el texto.\n","\n","**Fijate que** una vez que se instalaron todas, Google Colab te va a avisar que *se instalaron correctamente*. Si te pide reiniciar el entorno de ejecuci√≥n, hacelo."],"metadata":{"id":"Mz7iDu6KWfGg"}},{"cell_type":"code","source":["# @title Importando Librer√≠as y Configuracion de Modelos\n","\n","import torch\n","from transformers import BlipProcessor, BlipForConditionalGeneration\n","from transformers import pipeline\n","import gradio as gr\n","from PIL import Image\n","\n","# Configurando Nuestros Cerebros Digitales: Los Modelos\n","# Ahora que tenemos las librer√≠as instaladas, vamos a cargar los modelos de Inteligencia Artificial que van a hacer la magia.\n","# Pens√° en esto como \"descargar\" el cerebro que ya fue entrenado para las tareas que necesitamos.\n","\n","# Mir√° el c√≥digo:\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n","traductor = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")"],"metadata":{"id":"RlbWzaIAWa3j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**¬øQu√© estamos haciendo ac√°?**\n","\n","1.  **`BlipProcessor` y `BlipForConditionalGeneration`**: Estos son los componentes del modelo BLIP. El `processor` se encarga de preparar la imagen para que el modelo la entienda (por ejemplo, redimensionarla, normalizarla, etc.). El `model` es el que realmente \"lee\" la imagen y genera el texto. Estamos usando la versi√≥n `blip-image-captioning-large`, que es una de las m√°s potentes.\n","2.  **`pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")`**: Ac√° estamos usando la funci√≥n `pipeline` de `transformers` para crear un traductor. Le indicamos que queremos una tarea de `translation` (traducci√≥n) y especificamos el modelo `Helsinki-NLP/opus-mt-en-es`, que est√° entrenado para traducir de ingl√©s (`en`) a espa√±ol (`es`)."],"metadata":{"id":"UcHVuEv9W0KS"}},{"cell_type":"code","source":["# @title Funci√≥n `generar_descripcion`\n","\n","def generar_descripcion(imagen):\n","    \"\"\"\n","    Genera una descripci√≥n en espa√±ol para una imagen dada.\n","    \"\"\"\n","    # Paso 1: Procesar la imagen y generar la descripci√≥n en ingl√©s\n","    # 'processor' prepara la imagen para el modelo (la convierte a un formato que entiende).\n","    inputs = processor(imagen, return_tensors=\"pt\")\n","\n","    # 'model.generate' es el coraz√≥n del captioning.\n","    # Le pasamos los inputs procesados y le pedimos que genere hasta 50 palabras.\n","    out = model.generate(**inputs, max_length=50)\n","\n","    # 'processor.decode' convierte los n√∫meros que gener√≥ el modelo en texto legible.\n","    # 'skip_special_tokens=True' saca tokens que no son parte de la descripci√≥n.\n","    descripcion_en = processor.decode(out[0], skip_special_tokens=True)\n","\n","    # Paso 2: Traducir la descripci√≥n al espa√±ol\n","    # Usamos nuestro \"traductor\" para convertir la descripci√≥n de ingl√©s a espa√±ol.\n","    # [0]['translation_text'] es para extraer el texto de la traducci√≥n.\n","    descripcion_es = traductor(descripcion_en)[0]['translation_text']\n","\n","    # Paso 3: Devolver los resultados\n","    # La funci√≥n devuelve un diccionario con las descripciones en ambos idiomas.\n","    return {\n","        \"Descripci√≥n (English)\": descripcion_en,\n","        \"Descripci√≥n (Espa√±ol)\": descripcion_es\n","    }"],"metadata":{"id":"TqRQscgwW2bH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**¬øQu√© hace cada l√≠nea?**\n","\n","*   `inputs = processor(imagen, return_tensors=\"pt\")`: Ac√°, el `processor` toma la `imagen` que le pas√°s y la convierte a un formato (tensores de PyTorch, indicado por `\"pt\"`) que el modelo `BLIP` entiende. Es como \"preparar\" la foto para que el modelo la pueda \"leer\".\n","*   `out = model.generate(**inputs, max_length=50)`: Esta es la parte donde el modelo `BLIP` genera la descripci√≥n. Le pasamos los `inputs` (la imagen ya procesada) y le decimos `max_length=50` para que la descripci√≥n no sea m√°s larga de 50 palabras.\n","*   `descripcion_en = processor.decode(out[0], skip_special_tokens=True)`: El `model.generate` nos devuelve una secuencia de n√∫meros (tokens). El `processor.decode` convierte esos n√∫meros de nuevo en texto legible. Le decimos `skip_special_tokens=True` para que no incluya caracteres raros que usa el modelo internamente. As√≠ obtenemos la descripci√≥n en ingl√©s.\n","*   `descripcion_es = traductor(descripcion_en)[0]['translation_text']`: Ac√° entra en acci√≥n nuestro `traductor`. Le pasamos la `descripcion_en` y √©l se encarga de traducirla al espa√±ol. El `[0]['translation_text']` es para extraer el texto de la respuesta del traductor.\n","*   Finalmente, la funci√≥n devuelve un diccionario con ambas descripciones, as√≠ pod√©s ver tanto la original en ingl√©s como la traducida al espa√±ol."],"metadata":{"id":"s-fu406_XeMz"}},{"cell_type":"code","source":["# @title üåê Interfaz de Usuario\n","\n","# Para que nuestro sistema sea f√°cil de usar para cualquiera,\n","# vamos a crear una interfaz web simple con Gradio.\n","# Esto nos permite subir una imagen y ver los resultados en el navegador.\n","\n","# Mir√° c√≥mo armamos la interfaz:\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"\"\"\n","    # üñºÔ∏è Generador Autom√°tico de Descripciones de Im√°genes\n","    ¬°Sub√≠ una imagen y vas a ver c√≥mo el sistema le genera una descripci√≥n autom√°ticamente en espa√±ol!\n","    \"\"\")\n","\n","    with gr.Row():\n","        imagen_input = gr.Image(type=\"pil\", label=\"Sub√≠ tu imagen ac√°\")\n","\n","    with gr.Row():\n","        boton_analizar = gr.Button(\"Generar Descripci√≥n\")\n","\n","    with gr.Row():\n","        salida = gr.JSON(label=\"Resultados\")\n","\n","    boton_analizar.click(\n","        fn=generar_descripcion,\n","        inputs=imagen_input,\n","        outputs=salida\n","    )\n","\n","    gr.Markdown(\"\"\"\n","    ### üìù Instrucciones para Usar la Interfaz:\n","    1.  **Sub√≠ una imagen:** Us√° el campo de arriba que dice \"Sub√≠ tu imagen ac√°\". Pod√©s arrastrar y soltar una foto, o **clickear** para seleccionarla desde tu computadora.\n","    2.  **Generar la descripci√≥n:** Una vez que la imagen est√° cargada, **hac√© clic** en el bot√≥n \"Generar Descripci√≥n\".\n","    3.  **Observ√° los resultados:** Vas a ver un cuadro con formato JSON que te va a mostrar la descripci√≥n en ingl√©s y la traducci√≥n al espa√±ol.\n","\n","    ---\n","    ### ü§î Para Pensar un Poco: Reflexiones\n","\n","    Ahora que probaste el sistema, ¬°es momento de ponernos a pensar! Compart√≠ tus ideas y conclusiones.\n","\n","    *   ¬øQu√© tan precisas te parecieron las descripciones que gener√≥ el modelo? ¬øTe sorprendi√≥ alguna?\n","    *   ¬øQu√© tipo de detalles capt√≥ mejor el modelo en las im√°genes? ¬øY cu√°les no tanto?\n","    *   Si tuvieras que mejorar algo, ¬øc√≥mo cre√©s que se podr√≠a mejorar la calidad de las traducciones al espa√±ol?\n","    *   Pensando en el mundo real, ¬øqu√© aplicaciones pr√°cticas le ves a esta tecnolog√≠a? ¬øD√≥nde la usar√≠as?\n","    \"\"\")\n","\n","# üöÄ ¬°Listo para Probarlo!\n","# Ahora s√≠, ejecut√° esta √∫ltima celda de c√≥digo. Se va a abrir una URL (puede que tarde unos segundos).\n","# Hac√© clic en esa URL para abrir la interfaz de Gradio en tu navegador y ¬°empez√° a probar tu generador de descripciones!\n","demo.launch(share=True)"],"metadata":{"id":"2-LracSlXulZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üèÜ Conclusi√≥n: ¬°Lo Lograste!\n","\n","¬°Felicitaciones! üéâ Llegaste al final de esta clase y armaste tu propio sistema de generaci√≥n autom√°tica de descripciones de im√°genes. Hoy **aprendiste** c√≥mo los Modelos de Lenguaje-Visi√≥n (VLMs) pueden \"entender\" y \"describir\" el contenido de una imagen, y c√≥mo pod√©s usar la traducci√≥n autom√°tica para que esos resultados est√©n en espa√±ol.\n","\n","**Viste** c√≥mo se combinan diferentes modelos (uno para el captioning, otro para la traducci√≥n) y c√≥mo `Gradio` te permite armar una interfaz de usuario muy rapido.\n","\n","---\n","### üëâ Pr√≥ximos Pasos:\n","\n","La Inteligencia Artificial es un campo que crece muy rapido. Ac√° te dejo algunas ideas para que sigas investigando:\n","\n","*   **Experiment√° con otros modelos:** En Hugging Face ten√©s un mont√≥n de modelos de `Image Captioning` y `Traducci√≥n` para probar. ¬øHay alguno que funcione mejor?\n","*   **Mejor√° la traducci√≥n:** A veces las traducciones autom√°ticas no son perfectas. ¬øC√≥mo podr√≠as incorporar m√°s contexto o incluso usar modelos de traducci√≥n m√°s avanzados?\n","*   **Agreg√° funcionalidades:** ¬øQu√© tal si tu sistema pudiera identificar objetos espec√≠ficos en la imagen o responder preguntas sobre ella? Investig√° sobre `Object Detection` o `Visual Question Answering`.\n","*   **Optimiz√° el rendimiento:** Si quer√©s que tu modelo sea m√°s r√°pido, podr√≠as investigar t√©cnicas de cuantizaci√≥n o _distillation_."],"metadata":{"id":"Ih7nzc7tYT-9"}}]}